# 从网页中提取链接（BeautifulSoup）

> 原文： [https://pythonspot.com/extract-links-from-webpage-beautifulsoup/](https://pythonspot.com/extract-links-from-webpage-beautifulsoup/)

Web 抓取是从网站提取数据的技术。

模块 **BeautifulSoup** 设计用于 [**网页**](https://pythonspot.com/web-dev) 抓取。 BeautifulSoup 模块可以处理 HTML 和 XML。 它提供了用于搜索，导航和修改分析树的简单方法。

## 从网站获取链接

The example below prints all links on a webpage:

```py

from BeautifulSoup import BeautifulSoup
import urllib2
import re

html_page = urllib2.urlopen("https://arstechnica.com")
soup = BeautifulSoup(html_page)
for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
    print link.get('href')

```

它下载带有行的原始 html 代码：

```py

html_page = urllib2.urlopen("https://arstechnica.com")

```

创建了 BeautifulSoup 对象，我们使用该对象查找所有链接：

```py

soup = BeautifulSoup(html_page)
for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
    print link.get('href')

```

## 将网站链接提取到数组中

To store the links in an [**array**](https://pythonspot.com/python-lists/) you can use:

```py

from BeautifulSoup import BeautifulSoup
import urllib2
import re

html_page = urllib2.urlopen("https://arstechnica.com")
soup = BeautifulSoup(html_page)
links = []

for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
    links.append(link.get('href'))

print(links)

```

## 从网页提取链接的功能

If you repeatingly extract links you can use the function below:

```py

from BeautifulSoup import BeautifulSoup
import urllib2
import re

def getLinks(url):
    html_page = urllib2.urlopen(url)
    soup = BeautifulSoup(html_page)
    links = []

    for link in soup.findAll('a', attrs={'href': re.compile("^http://")}):
        links.append(link.get('href'))

    return links

print( getLinks("https://arstechnica.com") )

```

## 相关课程：

[使用 Python Selenium 的浏览器自动化](https://gum.co/GjuJxo)